{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bf4ba62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca5df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "def get_AppData(url):\n",
    "    \"\"\" Fetch HTML content from a given URL.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL of the webpage to fetch.\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup object if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raise an HTTPError if status is 4xx or 5xx\n",
    "        return BeautifulSoup(response.text, 'lxml')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch data for URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_general_details(soup):\n",
    "    \"\"\"Extract general details such as title, description, genre, and tags.\n",
    "\n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the game page.\n",
    "\n",
    "    Returns:\n",
    "        tuple: title, description, content, genre, player type, tags list, and release date.\n",
    "    \"\"\"\n",
    "    title = description = content = genre = player = tags_list = release_date = None\n",
    "\n",
    "    try:\n",
    "        title = soup.find('div', class_='apphub_AppName').get_text(strip=True)\n",
    "        description = soup.find('div', class_='game_description_snippet').get_text(strip=True)\n",
    "        content_div = soup.find('div', class_='shared_game_rating')\n",
    "        content = content_div.find('p').get_text(strip=True) if content_div else None\n",
    "        genre = [g.get_text(strip=True) for g in soup.select('div.details_block a')]\n",
    "        tags_list = [tag.get_text(strip=True) for tag in soup.select('div.glance_tags a')]\n",
    "        release_date = soup.find('div', class_='date').get_text(strip=True).replace(',', '')\n",
    "        player = soup.find('a', class_='game_area_details_specs_ctn').find('div', class_='label').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    return title, description, content, genre, player, tags_list, release_date\n",
    "\n",
    "\n",
    "def extract_details(div, index):\n",
    "    \"\"\"Extract developer or publisher details.\n",
    "    \n",
    "    Parameters:\n",
    "        div (list): List of div elements containing developer/publisher info.\n",
    "        index (int): Index of the div to extract details from.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (link, name) if found, else (None, None).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        detail_div = div[index].find('div', class_='summary column')\n",
    "        link = detail_div.find('a').get('href')\n",
    "        name = detail_div.get_text(strip=True)\n",
    "        return link, name\n",
    "    except (AttributeError, IndexError):\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def fetch_followers(link):\n",
    "    \"\"\"Fetch number of followers from a given developer/publisher link.\n",
    "    \n",
    "    Parameters:\n",
    "        link (str): URL of the developer/publisher page.\n",
    "    \n",
    "    Returns:\n",
    "        str: Number of followers or None if not found.\n",
    "    \"\"\"\n",
    "    soup = None\n",
    "    if not link:\n",
    "        return None\n",
    "    try:\n",
    "        soup = get_AppData(link)\n",
    "        res = soup.find('div', class_=\"num_followers\").get_text(strip=True).replace(',', '') if soup else None\n",
    "        return res\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_developer_publisher_details(soup):\n",
    "    \"\"\"Extract developer and publisher details, along with follower counts.\n",
    "    \n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the game page.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Developer and publisher names, and their follower counts.\n",
    "    \"\"\"\n",
    "    developer = publisher = dev_followers = pub_followers = None\n",
    "    try:\n",
    "        divs = soup.find_all('div', class_='dev_row')\n",
    "        dev_link, developer = extract_details(divs, 0)\n",
    "        pub_link, publisher = extract_details(divs, 1)\n",
    "        dev_followers = fetch_followers(dev_link)\n",
    "        pub_followers = fetch_followers(pub_link)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return developer, publisher, dev_followers, pub_followers\n",
    "\n",
    "\n",
    "def find_price(soup):\n",
    "    \"\"\"Extract price and discount prices from the game page.\n",
    "    \n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the game page.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Regular price and list of discount prices.\n",
    "    \"\"\"\n",
    "    price = discount_prices = None\n",
    "    try:\n",
    "        price = soup.find('div', class_='game_purchase_price').get_text(strip=True).replace('$', '')\n",
    "        discount_divs = soup.find_all('div', class_='discount_final_price')\n",
    "        discount_prices = [dp.get_text(strip=True).replace('$', '') for dp in discount_divs] or None\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return price, discount_prices\n",
    "\n",
    "\n",
    "def find_review_count(soup):\n",
    "    \"\"\"Extract monthly and all-time review counts and ratings.\n",
    "    \n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the game page.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Monthly review count, positive review ratio for the month, total review count, positive review ratio for all time.\n",
    "    \"\"\"\n",
    "    month_reviews = positive_review_ratio_month = total_reviews = positive_review_ratio_all_time = None\n",
    "\n",
    "    review_divs = soup.find_all('span', class_=\"nonresponsive_hidden responsive_reviewdesc\")\n",
    "        \n",
    "    try:\n",
    "        monthly_numbers = re.findall(r'\\d{1,3}(?:,\\d{3})*', review_divs[0].get_text())\n",
    "        positive_review_ratio_month = int(monthly_numbers[0].replace(',', ''))\n",
    "        month_reviews = int(monthly_numbers[1].replace(',', ''))\n",
    "    except (AttributeError, IndexError, ValueError):\n",
    "        month_reviews = positive_review_ratio_month = None\n",
    "        \n",
    "    try:\n",
    "        all_time_numbers = re.findall(r'\\d{1,3}(?:,\\d{3})*', review_divs[1].get_text())\n",
    "        positive_review_ratio_all_time = int(all_time_numbers[0].replace(',', ''))\n",
    "        total_reviews = int(all_time_numbers[1].replace(',', ''))\n",
    "    except (AttributeError, IndexError, ValueError):\n",
    "        total_reviews = positive_review_ratio_all_time = None\n",
    "\n",
    "    return month_reviews, positive_review_ratio_month, total_reviews, positive_review_ratio_all_time\n",
    "\n",
    "\n",
    "def find_media_links(soup):\n",
    "    \"\"\"Extract media links, including header image, screenshots, and videos.\n",
    "    \n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the game page.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Header URL, image URLs, thumbnail URLs, HD video URLs, and 480p video URLs.\n",
    "    \"\"\"\n",
    "    header_url = image_url_list = image_small_url_list = video_urls_hd_list = video_urls_480p_list = None\n",
    "    try:\n",
    "        header_url = soup.find('img', class_='game_header_image_full')['src']\n",
    "        image_url_list = [img['href'] for img in soup.find_all('a', class_='highlight_screenshot_link')]\n",
    "        image_small_url_list = [img.find('img')['src'] for img in soup.find_all('div', class_='highlight_strip_item highlight_strip_screenshot')]\n",
    "        video_links = soup.find_all('div', class_='highlight_player_item highlight_movie')\n",
    "        video_urls_hd_list = [v_link['data-mp4-hd-source'] for v_link in video_links if 'data-mp4-hd-source' in v_link.attrs]\n",
    "        video_urls_480p_list = [v_link['data-mp4-source'] for v_link in video_links if 'data-mp4-source' in v_link.attrs]\n",
    "    except (AttributeError, TypeError):\n",
    "        pass\n",
    "    return header_url, image_url_list, image_small_url_list, video_urls_hd_list, video_urls_480p_list\n",
    "\n",
    "\n",
    "def find_requirements(soup):\n",
    "    \"\"\"Extract system requirements.\n",
    "    \n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the game page.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of system requirements or None if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lines = soup.find('div', class_='sysreq_tabs').get_text(strip=True).split('\\n')\n",
    "        return [item.strip() for item in lines]\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_languages(soup):\n",
    "    \"\"\"Extract list of supported languages.\n",
    "    \n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the game page.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of supported languages.\n",
    "    \"\"\"\n",
    "    return [td.get_text(strip=True) for td in soup.select(\"td.ellipsis\")]\n",
    "\n",
    "\n",
    "def extract_data(appid) -> dict:\n",
    "    \"\"\"Main function to extract game details based on the appid from Steam.\"\"\"\n",
    "    \n",
    "    time.sleep(1) \n",
    "    url = f\"https://store.steampowered.com/app/{appid}/\"\n",
    "    soup = get_AppData(url) \n",
    "    \n",
    "    app_id = appid\n",
    "    title, description, content, genre, player, tags_list, release_date = find_general_details(soup)\n",
    "    developer, publisher, dev_followers, pub_followers = find_developer_publisher_details(soup) \n",
    "    price, discount_prices = find_price(soup)\n",
    "    month_reviews, pos_ratio_month, total_reviews, pos_ratio_all = find_review_count(soup)\n",
    "    header_url,image_url_list,image_small_url_list, video_urls_hd_list, video_urls_480p_list = find_media_links(soup)\n",
    "    sys_reqs = find_requirements(soup)\n",
    "    languages_list = find_languages(soup)\n",
    "    \n",
    "    # Combine all extracted details into a dictionary\n",
    "    game_data = {\n",
    "        'App ID': app_id,\n",
    "        'Title': title,\n",
    "        'Description': description,\n",
    "        'Content': content,\n",
    "        'Genre': genre,\n",
    "        'Player Type': player,\n",
    "        'Tags': tags_list,\n",
    "        'Release Date': release_date,\n",
    "        'Developer': developer,\n",
    "        'Publisher': publisher,\n",
    "        'Dev Followers': dev_followers,\n",
    "        'Pub Followers': pub_followers,\n",
    "        'Price': price,\n",
    "        'Discount Prices': discount_prices,\n",
    "        'Monthly Reviews': month_reviews,\n",
    "        'Positive Review Ratio (Monthly)': pos_ratio_month,\n",
    "        'Total Reviews': total_reviews,\n",
    "        'Positive Review Ratio (All Time)': pos_ratio_all,\n",
    "        'Header Image URL': header_url,\n",
    "        'Image URLs': image_url_list,\n",
    "        'Image Small URLs': image_small_url_list,\n",
    "        'HD Video URLs': video_urls_hd_list,\n",
    "        '480p Video URLs': video_urls_480p_list,\n",
    "        'System Requirements': sys_reqs,\n",
    "        'Languages': languages_list\n",
    "    }\n",
    "\n",
    "    return game_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfcfb321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T09:48:29.418621Z",
     "iopub.status.busy": "2024-10-27T09:48:29.418165Z",
     "iopub.status.idle": "2024-10-27T10:04:05.008894Z",
     "shell.execute_reply": "2024-10-27T10:04:05.007139Z",
     "shell.execute_reply.started": "2024-10-27T09:48:29.418564Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def save_batch_to_csv(batch_number, df, directory='scraped_data_1'):\n",
    "    \"\"\"Save a DataFrame to a CSV file in a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        batch_number (int): Batch identifier for the file name.\n",
    "        df (pd.DataFrame): DataFrame containing batch data.\n",
    "        directory (str): Directory path where the CSV file will be saved. Defaults to 'scraped_data_1'.\n",
    "    \"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    file_path = os.path.join(directory, f'steam_games_data_batch_{batch_number}.csv')\n",
    "    df.to_csv(file_path, index=False)\n",
    "    logging.info(f\"Batch {batch_number} saved successfully to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1a7f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='scraping_log.log', \n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4594c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_number, app_ids, max_workers=8, batch_pause=5):\n",
    "    \"\"\"Process a batch of app IDs, save results to CSV, and pause after each batch.\n",
    "\n",
    "    Parameters:\n",
    "        batch_number (int): Batch number for file naming.\n",
    "        app_ids (list): List of app IDs to process.\n",
    "        max_workers (int): Maximum number of threads for concurrent execution. Defaults to 10.\n",
    "        batch_pause (int): Number of seconds to wait after each batch. Defaults to 5 seconds.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting batch {batch_number} with {len(app_ids)} apps.\")\n",
    "    list_of_data = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(extract_data, app_id): app_id for app_id in app_ids}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Processing Batch {batch_number}\"):\n",
    "            app_id = futures[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                if data:  # Add valid results to the list\n",
    "                    list_of_data.append(data)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing app ID {app_id}: {e}\")\n",
    "    \n",
    "    if list_of_data:\n",
    "        df = pd.DataFrame(list_of_data)\n",
    "        save_batch_to_csv(batch_number, df)\n",
    "        logging.info(f\"Completed processing batch {batch_number}\")\n",
    "    else:\n",
    "        logging.warning(f\"No data retrieved for batch {batch_number}\")\n",
    "\n",
    "    # Pause after each batch\n",
    "    logging.info(f\"Pausing for {batch_pause} seconds before the next batch.\")\n",
    "    time.sleep(batch_pause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc397416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files(dir_path):\n",
    "    \"\"\"Count the number of files in a given directory.\n",
    "\n",
    "    Parameters:\n",
    "        dir_path (str): Path to the directory.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of files in the directory.\n",
    "    \"\"\"\n",
    "    return sum(1 for path in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48e48d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of CSV files\n",
    "files = ['newandtrending.csv', 'discounted.csv', 'comingsoon.csv', 'toprated.csv', 'topsellers.csv']\n",
    "\n",
    "# Dictionary to store the distinct lists for each file\n",
    "column_data = {}\n",
    "\n",
    "# Process each file\n",
    "for f in files:\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "        # Extract the first column and remove duplicates\n",
    "        first_column = list(set(df.iloc[:, 0]))\n",
    "        \n",
    "        # Store the list in the dictionary with the filename as the key\n",
    "        column_data[f] = first_column\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {f}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(column_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfc14a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = set()\n",
    "for col in df.columns:\n",
    "    unique_ids.update(df[col])\n",
    "\n",
    "unique_list = list(unique_ids)\n",
    "\n",
    "#print(\"Unique list of values:\", unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47c3bd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 2:   0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 2: 100%|██████████| 300/300 [05:52<00:00,  1.18s/it]\n",
      "Processing Batch 3: 100%|██████████| 300/300 [04:26<00:00,  1.13it/s]\n",
      "Processing Batch 4: 100%|██████████| 96/96 [01:27<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the starting batch and batch size\n",
    "starting_batch = count_files('./scraped_data_1')\n",
    "print()\n",
    "batch_size = 300\n",
    "a = starting_batch * batch_size\n",
    "b = len(unique_list)\n",
    "\n",
    "# Process each batch within the range of IDs\n",
    "for batch_start in range(a, b, batch_size):\n",
    "    batch_number = batch_start // batch_size\n",
    "    ids = unique_list[batch_start:batch_start + batch_size]\n",
    "    process_batch(batch_number, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad0fe8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96fc6ae",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = glob.glob('./scraped_data_1/*.csv')\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through the list of CSV files and concatenate them\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    merged_df = pd.concat([merged_df, df])\n",
    "\n",
    "# Drop duplicate rows\n",
    "merged_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('./scraped_data_1/merged_data.csv', index=False)\n",
    "# Ensure no duplicates based on the 'ID' column\n",
    "merged_df = merged_df.drop_duplicates(subset='ID')\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('./scraped_data_1/merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a6bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304dbafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac4e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d4666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b23e505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518f129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88565f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5783d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57b6dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f84d784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6922e274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac1f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb3f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5cedac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab3bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ca6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d832b512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22e4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bdf92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90868d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ddde3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c5091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba1a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda26fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c3e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b05c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c041e99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a20d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447eee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098b0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb54f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe98588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82805cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd71b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09e488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225cd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff5f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8906813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00043202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875cf0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1198d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c4a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
