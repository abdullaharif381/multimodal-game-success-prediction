{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:08:51.715849Z",
     "iopub.status.busy": "2024-10-27T10:08:51.715354Z",
     "iopub.status.idle": "2024-10-27T10:08:51.722878Z",
     "shell.execute_reply": "2024-10-27T10:08:51.721450Z",
     "shell.execute_reply.started": "2024-10-27T10:08:51.715802Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import webdriver_manager\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:10:09.885326Z",
     "iopub.status.busy": "2024-10-27T10:10:09.884847Z",
     "iopub.status.idle": "2024-10-27T10:10:09.893595Z",
     "shell.execute_reply": "2024-10-27T10:10:09.892211Z",
     "shell.execute_reply.started": "2024-10-27T10:10:09.885286Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_AppList(version):\n",
    "    url = f\"https://api.steampowered.com/ISteamApps/GetAppList/{version}/\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        apps = data['applist']['apps']\n",
    "        if version == 'v2':\n",
    "            return apps            \n",
    "        else:\n",
    "            return apps['app']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INDIE GAMES SCRIPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--headless')  # Optional: Run in headless mode\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# Initialize WebDriver with Chrome options\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "def process_url(url, output_file, show_more_clicks=20):\n",
    "    \"\"\"\n",
    "    Process a Steam URL, click \"Show More\" button specified times, and extract game IDs.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the Steam page to process.\n",
    "        output_file (str): Name of the CSV file to save game IDs.\n",
    "        show_more_clicks (int): Number of times to click \"Show More\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the target page\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        # Click the \"Show More\" button\n",
    "        for i in range(show_more_clicks):\n",
    "            try:\n",
    "                show_more_button = wait.until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"SaleSection_13268\"]//button[contains(text(), \"Show more\")]'))\n",
    "                )\n",
    "                show_more_button.click()\n",
    "                print(f\"'Show More' button clicked {i + 1} times.\")\n",
    "                time.sleep(2)  # Wait for new content to load\n",
    "            except Exception as e:\n",
    "                print(f\"Error while clicking the button: {e}\")\n",
    "                break  # Stop if the button is not clickable or present\n",
    "\n",
    "        # Calculate the number of games to extract\n",
    "        total_games_to_extract = show_more_clicks * 12\n",
    "        print(f\"Total games to extract: {total_games_to_extract}\")\n",
    "\n",
    "        # Prepare to store extracted game IDs\n",
    "        game_ids = []\n",
    "\n",
    "        # Function to extract game ID from the game URL\n",
    "        def extract_game_id(game_url):\n",
    "            if '/app/' in game_url:\n",
    "                return game_url.split('/app/')[1].split('/')[0]\n",
    "            return None\n",
    "\n",
    "        # Start extracting game IDs\n",
    "        for game_index in range(1, total_games_to_extract + 1):\n",
    "            game_xpath = f'//*[@id=\"SaleSection_13268\"]/div[2]/div[2]/div[2]/div[2]/div/div[{game_index}]/div/div/div/div/div[2]/div[2]/a'\n",
    "            try:\n",
    "                game_element = driver.find_element(By.XPATH, game_xpath)\n",
    "                game_url = game_element.get_attribute('href')\n",
    "                game_id = extract_game_id(game_url)\n",
    "                if game_id:\n",
    "                    game_ids.append(game_id)\n",
    "                    print(f\"Game ID {game_id} extracted.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting game at index {game_index}: {e}\")\n",
    "                break\n",
    "\n",
    "        # Save extracted game IDs to a CSV file\n",
    "        with open(output_file, \"w\", newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Game ID\"])  # Header row\n",
    "            for game_id in game_ids:\n",
    "                writer.writerow([game_id])\n",
    "        print(f\"{len(game_ids)} Game IDs extracted and saved to '{output_file}'.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "\n",
    "\n",
    "# URLs to process\n",
    "urls = [\n",
    "    (\"https://store.steampowered.com/tags/en/Indie/?flavor=contenthub_newandtrending\", \"newandtrending.csv\"),\n",
    "    (\"https://store.steampowered.com/tags/en/Indie/?flavor=contenthub_topsellers\", \"topsellers.csv\"),\n",
    "    (\"https://store.steampowered.com/tags/en/Indie/?flavor=contenthub_toprated\", \"toprated.csv\"),\n",
    "    (\"https://store.steampowered.com/tags/en/Indie/?flavor=popularpurchaseddiscounted\", \"discounted.csv\"),\n",
    "    (\"https://store.steampowered.com/tags/en/Indie/?flavor=popularcomingsoon\", \"comingsoon.csv\"),\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Loop through each URL and process\n",
    "    for url, output_file in urls:\n",
    "        print(f\"Processing URL: {url}\")\n",
    "        process_url(url, output_file, show_more_clicks=25)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing tab 'New & Trending': HTTPConnectionPool(host='localhost', port=51508): Max retries exceeded with url: /session/344e6f9687e670c22ff0ab2969c93b9c/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001BB63578B80>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "Error processing tab 'Top Sellers': HTTPConnectionPool(host='localhost', port=51508): Max retries exceeded with url: /session/344e6f9687e670c22ff0ab2969c93b9c/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001BB634E1640>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "Error processing tab 'Top Rated': HTTPConnectionPool(host='localhost', port=51508): Max retries exceeded with url: /session/344e6f9687e670c22ff0ab2969c93b9c/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001BB63594790>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "Error processing tab 'Discounted': HTTPConnectionPool(host='localhost', port=51508): Max retries exceeded with url: /session/344e6f9687e670c22ff0ab2969c93b9c/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001BB63594700>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n"
     ]
    }
   ],
   "source": [
    "url = \"https://store.steampowered.com/tags/en/Indie/\"\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(url) \n",
    "for tab_name, file_name in tabs.items():\n",
    "    try:\n",
    "        # Use JavaScript click to avoid Selenium click issues\n",
    "        tab = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, f\"//div[text()='{tab_name}']\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].click();\", tab)\n",
    "        time.sleep(3)  # Wait for tab content to load\n",
    "\n",
    "        game_ids = set()  # Use a set to avoid duplicate IDs\n",
    "\n",
    "        # Scroll down and click \"Show More\" until no more games are loading\n",
    "        while True:\n",
    "            # Find all game ID elements on the page\n",
    "            games = driver.find_elements(By.XPATH, \"//a[contains(@href, '/app/')]\")\n",
    "            for game in games:\n",
    "                try:\n",
    "                    game_id = game.get_attribute(\"href\").split(\"/app/\")[-1].split(\"/\")[0]\n",
    "                    game_ids.add(game_id)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting game ID: {e}\")\n",
    "\n",
    "            # Click \"Show More\" button if available\n",
    "            try:\n",
    "                show_more_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, \"//button[text()='Show More']\"))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", show_more_button)\n",
    "                time.sleep(2)  # Wait for new items to load\n",
    "            except:\n",
    "                print(f\"No more games to load for {tab_name}.\")\n",
    "                break\n",
    "\n",
    "        # Save game IDs to a CSV file\n",
    "        pd.DataFrame(game_ids, columns=['GameID']).to_csv(file_name, index=False)\n",
    "        print(f\"Saved {len(game_ids)} IDs for {tab_name} to {file_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing tab '{tab_name}': {e}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:15:15.555621Z",
     "iopub.status.busy": "2024-10-27T10:15:15.555076Z",
     "iopub.status.idle": "2024-10-27T10:15:17.812479Z",
     "shell.execute_reply": "2024-10-27T10:15:17.811325Z",
     "shell.execute_reply.started": "2024-10-27T10:15:15.555572Z"
    }
   },
   "outputs": [],
   "source": [
    "v1 = 'v1'\n",
    "v2 = 'v2'\n",
    "\n",
    "# apps = get_AppList(v1)\n",
    "# df1 = pd.DataFrame(apps)\n",
    "\n",
    "# apps = get_AppList(v2)\n",
    "# df2 = pd.DataFrame(apps)\n",
    "\n",
    "# app_ids_df = pd.concat([df1, df2], ignore_index=True).drop_duplicates(subset='appid')  \n",
    "# app_ids_df.to_csv('all_steam_games.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:15:24.867594Z",
     "iopub.status.busy": "2024-10-27T10:15:24.866673Z",
     "iopub.status.idle": "2024-10-27T10:15:24.880606Z",
     "shell.execute_reply": "2024-10-27T10:15:24.879170Z",
     "shell.execute_reply.started": "2024-10-27T10:15:24.867543Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# app_ids_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:10:12.379187Z",
     "iopub.status.busy": "2024-10-27T10:10:12.378715Z",
     "iopub.status.idle": "2024-10-27T10:10:12.391042Z",
     "shell.execute_reply": "2024-10-27T10:10:12.389778Z",
     "shell.execute_reply.started": "2024-10-27T10:10:12.379140Z"
    }
   },
   "outputs": [],
   "source": [
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:10:12.393855Z",
     "iopub.status.busy": "2024-10-27T10:10:12.392849Z",
     "iopub.status.idle": "2024-10-27T10:10:12.407264Z",
     "shell.execute_reply": "2024-10-27T10:10:12.405853Z",
     "shell.execute_reply.started": "2024-10-27T10:10:12.393791Z"
    }
   },
   "outputs": [],
   "source": [
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:21:46.681523Z",
     "iopub.status.busy": "2024-10-27T10:21:46.680432Z",
     "iopub.status.idle": "2024-10-27T10:21:46.688127Z",
     "shell.execute_reply": "2024-10-27T10:21:46.686708Z",
     "shell.execute_reply.started": "2024-10-27T10:21:46.681459Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_AppData(url):\n",
    "    \"\"\"Fetch HTML content from a given URL.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL of the webpage to fetch.\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup object if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raise an HTTPError if status is 4xx or 5xx\n",
    "        return BeautifulSoup(response.text, 'lxml')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch data for URL {url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:10:12.419338Z",
     "iopub.status.busy": "2024-10-27T10:10:12.418952Z",
     "iopub.status.idle": "2024-10-27T10:10:12.434578Z",
     "shell.execute_reply": "2024-10-27T10:10:12.433355Z",
     "shell.execute_reply.started": "2024-10-27T10:10:12.419299Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_general_details(soup):\n",
    "    \"\"\"Extract general details such as title, description, genre, and tags.\n",
    "\n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the game page.\n",
    "\n",
    "    Returns:\n",
    "        tuple: title, description, content, genre, player type, tags list, and release date.\n",
    "    \"\"\"\n",
    "    title = description = content = genre = player = tags_list = release_date = None\n",
    "\n",
    "    try:\n",
    "        title = soup.find('div', class_='apphub_AppName').get_text(strip=True)\n",
    "        description = soup.find('div', class_='game_description_snippet').get_text(strip=True)\n",
    "        content_div = soup.find('div', class_='shared_game_rating')\n",
    "        content = content_div.find('p').get_text(strip=True) if content_div else None\n",
    "        genre = [g.get_text(strip=True) for g in soup.select('div.details_block a')]\n",
    "        tags_list = [tag.get_text(strip=True) for tag in soup.select('div.glance_tags a')]\n",
    "        release_date = soup.find('div', class_='date').get_text(strip=True).replace(',', '')\n",
    "        player = soup.find('a', class_='game_area_details_specs_ctn').find('div', class_='label').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    return title, description, content, genre, player, tags_list, release_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:10:13.079158Z",
     "iopub.status.busy": "2024-10-27T10:10:13.078693Z",
     "iopub.status.idle": "2024-10-27T10:10:13.091336Z",
     "shell.execute_reply": "2024-10-27T10:10:13.090039Z",
     "shell.execute_reply.started": "2024-10-27T10:10:13.079114Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_details(div, index):\n",
    "    \"\"\"Extract developer or publisher details.\n",
    "    \n",
    "    Parameters:\n",
    "        div (list): List of div elements containing developer/publisher info.\n",
    "        index (int): Index of the div to extract details from.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (link, name) if found, else (None, None).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        detail_div = div[index].find('div', class_='summary column')\n",
    "        link = detail_div.find('a').get('href')\n",
    "        name = detail_div.get_text(strip=True)\n",
    "        return link, name\n",
    "    except (AttributeError, IndexError):\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_followers(link):\n",
    "    \"\"\"Fetch number of followers from a given developer/publisher link.\n",
    "    \n",
    "    Parameters:\n",
    "        link (str): URL of the developer/publisher page.\n",
    "    \n",
    "    Returns:\n",
    "        str: Number of followers or None if not found.\n",
    "    \"\"\"\n",
    "    soup = None\n",
    "    if not link:\n",
    "        return None\n",
    "    try:\n",
    "        soup = get_AppData(link)\n",
    "        res = soup.find('div', class_=\"num_followers\").get_text(strip=True).replace(',', '') if soup else None\n",
    "        return res\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_developer_publisher_details(soup):\n",
    "    \"\"\"Extract developer and publisher details, along with follower counts.\n",
    "    \n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the game page.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Developer and publisher names, and their follower counts.\n",
    "    \"\"\"\n",
    "    developer = publisher = dev_followers = pub_followers = None\n",
    "    try:\n",
    "        divs = soup.find_all('div', class_='dev_row')\n",
    "        dev_link, developer = extract_details(divs, 0)\n",
    "        pub_link, publisher = extract_details(divs, 1)\n",
    "        dev_followers = fetch_followers(dev_link)\n",
    "        pub_followers = fetch_followers(pub_link)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return developer, publisher, dev_followers, pub_followers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:24:00.755074Z",
     "iopub.status.busy": "2024-10-27T10:24:00.754532Z",
     "iopub.status.idle": "2024-10-27T10:24:00.761519Z",
     "shell.execute_reply": "2024-10-27T10:24:00.760122Z",
     "shell.execute_reply.started": "2024-10-27T10:24:00.755028Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_price(soup):\n",
    "    \"\"\"Extract price and discount prices from the game page.\n",
    "    \n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the game page.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Regular price and list of discount prices.\n",
    "    \"\"\"\n",
    "    price = discount_prices = None\n",
    "    try:\n",
    "        price = soup.find('div', class_='game_purchase_price').get_text(strip=True).replace('$', '')\n",
    "        discount_divs = soup.find_all('div', class_='discount_final_price')\n",
    "        discount_prices = [dp.get_text(strip=True).replace('$', '') for dp in discount_divs] or None\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return price, discount_prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:19:33.237270Z",
     "iopub.status.busy": "2024-10-27T10:19:33.236800Z",
     "iopub.status.idle": "2024-10-27T10:19:33.247941Z",
     "shell.execute_reply": "2024-10-27T10:19:33.246630Z",
     "shell.execute_reply.started": "2024-10-27T10:19:33.237224Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1949018397.py, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 35\u001b[1;36m\u001b[0m\n\u001b[1;33m    return month_reviews, positive_review_ratio_month, total_reviews, positive_review_ratio_all_time\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def find_review_count(soup):\n",
    "    \"\"\"Extract monthly and all-time review counts and ratings.\n",
    "    \n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the game page.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Monthly review count, positive review ratio for the month, total review count, positive review ratio for all time.\n",
    "    \"\"\"\n",
    "    month_reviews = positive_review_ratio_month = total_reviews = positive_review_ratio_all_time = None\n",
    "\n",
    "    \n",
    "    review_divs = soup.find_all('span', class_=\"nonresponsive_hidden responsive_reviewdesc\")\n",
    "        \n",
    "    try:\n",
    "        monthly_numbers = re.findall(r'\\d{1,3}(?:,\\d{3})*', review_divs[0].get_text())\n",
    "        positive_review_ratio_month = int(monthly_numbers[0].replace(',', ''))\n",
    "        month_reviews = int(monthly_numbers[1].replace(',', ''))\n",
    "    except (AttributeError, IndexError, ValueError):\n",
    "        month_reviews = positive_review_ratio_month = None\n",
    "        \n",
    "    try:\n",
    "        all_time_numbers = re.findall(r'\\d{1,3}(?:,\\d{3})*', review_divs[1].get_text())\n",
    "        positive_review_ratio_all_time = int(all_time_numbers[0].replace(',', ''))\n",
    "        total_reviews = int(all_time_numbers[1].replace(',', ''))\n",
    "    except (AttributeError, IndexError, ValueError):\n",
    "        total_reviews = positive_review_ratio_all_time = None\n",
    "\n",
    "    \n",
    "    return month_reviews, positive_review_ratio_month, total_reviews, positive_review_ratio_all_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:19:34.056256Z",
     "iopub.status.busy": "2024-10-27T10:19:34.055816Z",
     "iopub.status.idle": "2024-10-27T10:19:34.066569Z",
     "shell.execute_reply": "2024-10-27T10:19:34.065369Z",
     "shell.execute_reply.started": "2024-10-27T10:19:34.056216Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_media_links(soup):\n",
    "    \"\"\"Extract media links, including header image, screenshots, and videos.\n",
    "    \n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the game page.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Header URL, image URLs, thumbnail URLs, HD video URLs, and 480p video URLs.\n",
    "    \"\"\"\n",
    "    header_url = image_url_list = image_small_url_list = video_urls_hd_list = video_urls_480p_list = None\n",
    "    try:\n",
    "        header_url = soup.find('img', class_='game_header_image_full')['src']\n",
    "        image_url_list = [img['href'] for img in soup.find_all('a', class_='highlight_screenshot_link')]\n",
    "        image_small_url_list = [img.find('img')['src'] for img in soup.find_all('div', class_='highlight_strip_item highlight_strip_screenshot')]\n",
    "        video_links = soup.find_all('div', class_='highlight_player_item highlight_movie')\n",
    "        video_urls_hd_list = [v_link['data-mp4-hd-source'] for v_link in video_links if 'data-mp4-hd-source' in v_link.attrs]\n",
    "        video_urls_480p_list = [v_link['data-mp4-source'] for v_link in video_links if 'data-mp4-source' in v_link.attrs]\n",
    "    except (AttributeError, TypeError):\n",
    "        pass\n",
    "    return header_url, image_url_list, image_small_url_list, video_urls_hd_list, video_urls_480p_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:19:34.248939Z",
     "iopub.status.busy": "2024-10-27T10:19:34.248431Z",
     "iopub.status.idle": "2024-10-27T10:19:34.256684Z",
     "shell.execute_reply": "2024-10-27T10:19:34.255140Z",
     "shell.execute_reply.started": "2024-10-27T10:19:34.248891Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_requirements(soup):\n",
    "    \"\"\"Extract system requirements.\n",
    "    \n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the game page.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of system requirements or None if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lines = soup.find('div', class_='sysreq_tabs').get_text(strip=True).split('\\n')\n",
    "        return [item.strip() for item in lines]\n",
    "    except AttributeError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:19:34.593572Z",
     "iopub.status.busy": "2024-10-27T10:19:34.592253Z",
     "iopub.status.idle": "2024-10-27T10:19:34.599279Z",
     "shell.execute_reply": "2024-10-27T10:19:34.597869Z",
     "shell.execute_reply.started": "2024-10-27T10:19:34.593407Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_languages(soup):\n",
    "    \"\"\"Extract list of supported languages.\n",
    "    \n",
    "    Parameters:\n",
    "        soup (BeautifulSoup): Parsed HTML content of the game page.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of supported languages.\n",
    "    \"\"\"\n",
    "    return [td.get_text(strip=True) for td in soup.select(\"td.ellipsis\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T10:19:35.056269Z",
     "iopub.status.busy": "2024-10-27T10:19:35.055305Z",
     "iopub.status.idle": "2024-10-27T10:19:35.068392Z",
     "shell.execute_reply": "2024-10-27T10:19:35.067081Z",
     "shell.execute_reply.started": "2024-10-27T10:19:35.056216Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_data(appid):\n",
    "    \n",
    "    time.sleep(1) \n",
    "    url = f\"https://store.steampowered.com/app/{appid}/\"\n",
    "    soup = get_AppData(url) \n",
    "    \n",
    "    app_id = appid\n",
    "    title, description, content, genre, player, tags_list, release_date = find_general_details(soup)\n",
    "    developer, publisher, dev_followers, pub_followers = find_developer_publisher_details(soup) \n",
    "    price, discount_prices = find_price(soup)\n",
    "    month_reviews, pos_ratio_month, total_reviews, pos_ratio_all = find_review_count(soup)\n",
    "    header_url,image_url_list,image_small_url_list, video_urls_hd_list, video_urls_480p_list = find_media_links(soup)   \n",
    "    software = find_requirements(soup)\n",
    "    languages = find_languages(soup)\n",
    "        \n",
    "    data = {\n",
    "        'app_id': app_id,\n",
    "        'title': title,\n",
    "        'description': description,\n",
    "        'content': content,\n",
    "        'developer': developer,\n",
    "        'publisher': publisher,\n",
    "        'dev_followers':dev_followers,\n",
    "        'pub_followers' : pub_followers,\n",
    "        'genre': genre,\n",
    "        'release_date': release_date,\n",
    "        'price_usd': price,\n",
    "        'discount_price':discount_prices,\n",
    "        'software': software,\n",
    "        'player': player,\n",
    "        'languages' : languages,\n",
    "        'month_reviews': month_reviews,\n",
    "        'positive_ratio_month': pos_ratio_month,  \n",
    "        'total_reviews': total_reviews,\n",
    "        'positive_ratio_all': pos_ratio_all,\n",
    "        'tags_list': tags_list,\n",
    "        'header_url': header_url,\n",
    "        'image_url_list': image_url_list,\n",
    "        'image_small_url_list': image_small_url_list,\n",
    "        'video_urls_hd_list': video_urls_hd_list,\n",
    "        'video_urls_480p_list': video_urls_480p_list\n",
    "    }\n",
    "\n",
    "    \n",
    "    return data\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_id = ['2878980', '10', '1293830', '306130']\n",
    "for i in test_id:\n",
    "    display(extract_data(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T09:48:29.418621Z",
     "iopub.status.busy": "2024-10-27T09:48:29.418165Z",
     "iopub.status.idle": "2024-10-27T10:04:05.008894Z",
     "shell.execute_reply": "2024-10-27T10:04:05.007139Z",
     "shell.execute_reply.started": "2024-10-27T09:48:29.418564Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def save_batch_to_csv(batch_number, df, directory='scraped_data_1'):\n",
    "    \"\"\"Save a DataFrame to a CSV file in a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        batch_number (int): Batch identifier for the file name.\n",
    "        df (pd.DataFrame): DataFrame containing batch data.\n",
    "        directory (str): Directory path where the CSV file will be saved. Defaults to 'scraped_data_1'.\n",
    "    \"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    file_path = os.path.join(directory, f'steam_games_data_batch_{batch_number}.csv')\n",
    "    df.to_csv(file_path, index=False)\n",
    "    logging.info(f\"Batch {batch_number} saved successfully to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='scraping_log.log', \n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_number, app_ids, max_workers=8, batch_pause=5):\n",
    "    \"\"\"Process a batch of app IDs, save results to CSV, and pause after each batch.\n",
    "\n",
    "    Parameters:\n",
    "        batch_number (int): Batch number for file naming.\n",
    "        app_ids (list): List of app IDs to process.\n",
    "        max_workers (int): Maximum number of threads for concurrent execution. Defaults to 10.\n",
    "        batch_pause (int): Number of seconds to wait after each batch. Defaults to 5 seconds.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting batch {batch_number} with {len(app_ids)} apps.\")\n",
    "    list_of_data = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(extract_data, app_id): app_id for app_id in app_ids}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Processing Batch {batch_number}\"):\n",
    "            app_id = futures[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                if data:  # Add valid results to the list\n",
    "                    list_of_data.append(data)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing app ID {app_id}: {e}\")\n",
    "    \n",
    "    if list_of_data:\n",
    "        df = pd.DataFrame(list_of_data)\n",
    "        save_batch_to_csv(batch_number, df)\n",
    "        logging.info(f\"Completed processing batch {batch_number}\")\n",
    "    else:\n",
    "        logging.warning(f\"No data retrieved for batch {batch_number}\")\n",
    "\n",
    "    # Pause after each batch\n",
    "    logging.info(f\"Pausing for {batch_pause} seconds before the next batch.\")\n",
    "    time.sleep(batch_pause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files(dir_path):\n",
    "    \"\"\"Count the number of files in a given directory.\n",
    "\n",
    "    Parameters:\n",
    "        dir_path (str): Path to the directory.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of files in the directory.\n",
    "    \"\"\"\n",
    "    return sum(1 for path in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the starting batch and batch size\n",
    "starting_batch = count_files('./scraped_data_1')\n",
    "batch_size = 1000\n",
    "a = starting_batch * batch_size\n",
    "b = len(app_ids_df)\n",
    "\n",
    "# Process each batch within the range of IDs\n",
    "for batch_start in range(a, b, batch_size):\n",
    "    batch_number = batch_start // batch_size\n",
    "    ids = app_ids_df['appid'][batch_start:batch_start + batch_size].to_list()\n",
    "    process_batch(batch_number, ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_top_100.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-27T10:04:05.010128Z",
     "iopub.status.idle": "2024-10-27T10:04:05.010584Z",
     "shell.execute_reply": "2024-10-27T10:04:05.010367Z",
     "shell.execute_reply.started": "2024-10-27T10:04:05.010346Z"
    }
   },
   "outputs": [],
   "source": [
    "df_100 = pd.read_csv('top_100_games.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-27T10:04:05.012431Z",
     "iopub.status.idle": "2024-10-27T10:04:05.012914Z",
     "shell.execute_reply": "2024-10-27T10:04:05.012723Z",
     "shell.execute_reply.started": "2024-10-27T10:04:05.012697Z"
    }
   },
   "outputs": [],
   "source": [
    "df1_unique = app_ids_df.drop_duplicates(subset='appid')\n",
    "df2_unique = df_100.drop_duplicates(subset='appid')\n",
    "common_apps = pd.merge(df1_unique, df2_unique, on='appid', how='inner').sort_values(by='rank', ascending=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://store.steampowered.com/charts/topselling/global\" \n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(url) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbody_element = driver.find_element(By.TAG_NAME, \"tbody\")\n",
    "tbody_html = tbody_element.get_attribute(\"outerHTML\")\n",
    "soup = BeautifulSoup(tbody_html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_ids = []\n",
    "ranks = []\n",
    "urls = []\n",
    "\n",
    "# Loop through each row in the <tbody> tag\n",
    "for row in soup.find_all('tr', class_=\"_2-RN6nWOY56sNmcDHu069P\"):\n",
    "    # Extract the rank\n",
    "    rank = row.find('td', class_=\"_34h48M_x9S-9Q2FFPX_CcU\").get_text(strip=True)\n",
    "    ranks.append(rank)\n",
    "    \n",
    "    # Extract the URL and app id\n",
    "    link = row.find('a', class_=\"_2C5PJOUH6RqyuBNEwaCE9X\")\n",
    "    if link:\n",
    "        url = link['href']\n",
    "        urls.append(url)\n",
    "        \n",
    "        # Extract app id from the URL using regex\n",
    "        app_id = re.search(r'/app/(\\d+)/', url)\n",
    "        app_ids.append(app_id.group(1) if app_id else None)\n",
    "\n",
    "# Create a DataFrame\n",
    "df_top_100 = pd.DataFrame({\n",
    "    'appid': app_ids,\n",
    "    'rank': ranks,\n",
    "    'url': urls\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_top_100.to_csv(\"top_100_games.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to top_100_games.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5956909,
     "sourceId": 9733603,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5957106,
     "sourceId": 9733864,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
